{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'timestamp',\n",
       " 'product_id',\n",
       " 'product_department',\n",
       " 'product_category',\n",
       " 'card_id',\n",
       " 'user_id',\n",
       " 'C15',\n",
       " 'C16',\n",
       " 'C17',\n",
       " 'C18',\n",
       " 'C19',\n",
       " 'C20',\n",
       " 'C21',\n",
       " 'amount',\n",
       " 'isfraud']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train_v2.csv')\n",
    "columns = list(train_data)\n",
    "N_original, M_original = train_data.shape\n",
    "columns, N_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline\n",
    "* With trees, normalizing features is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from include.DatetimeFromTimestamp import DatetimeFromTimestamp\n",
    "from include.DataFrameDropper import DataFrameDropper\n",
    "from include.DataFrameSelector import DataFrameSelector\n",
    "from include.FilterNMostCommon import FilterNMostCommon\n",
    "\n",
    "columns_to_drop = ['id', 'timestamp', 'product_id', 'product_department', 'product_category', 'card_id', 'user_id', 'datetime']\n",
    "\n",
    "pipeline_normal = Pipeline([\n",
    "    ('datetime_creator', DatetimeFromTimestamp()),\n",
    "    ('dataframe_dropper', DataFrameDropper(attribute_names=columns_to_drop)),\n",
    "])\n",
    "\n",
    "pipeline_1hot = Pipeline([\n",
    "    ('dataframe_selector', DataFrameSelector(['product_category'])),\n",
    "    ('filter_n_most_common', FilterNMostCommon(N=5, attribute_name='product_category')),\n",
    "    ('1hot_encoder', OneHotEncoder(sparse = False))\n",
    "])\n",
    "\n",
    "pipeline_full = FeatureUnion(transformer_list=[\n",
    "    ('pipeline_normal', pipeline_normal),\n",
    "    ('pipeline_1hot', pipeline_1hot),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test split\n",
    "* Normally I would use random sampling, stratified by an attribute of major relevance, however, in this case the test data that was given follows the train data in time. Therefore, in order to do local testing my first guess would be that it is better to remake that scenario and sample the data by simply splitting it sorted as it is, by time.\n",
    "* Cross validation is not necessary given that we have a test set big enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99999, 15), (99999,), (100000, 15), (100000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by = 2\n",
    "N_train = 100000\n",
    "\n",
    "start_at = N_original - N_train * split_by\n",
    "split_at = start_at + N_train\n",
    "\n",
    "train_X = pd.DataFrame(train_data.iloc[start_at:split_at-1,:-1])\n",
    "train_Y = train_data.iloc[start_at:split_at-1,-1]\n",
    "test_X = pd.DataFrame(train_data.iloc[split_at:,:-1])\n",
    "test_Y = train_data.iloc[split_at:,-1]\n",
    "\n",
    "train_X.shape, train_Y.shape, test_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_treated = pipeline_full.fit_transform(train_X)\n",
    "test_X_treated = pipeline_full.transform(test_X)\n",
    "\n",
    "train_X_treated.shape, test_X_treated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7598506864737222"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "current_model = DecisionTreeClassifier\n",
    "\n",
    "model = current_model()\n",
    "model.fit(train_X_treated, train_Y)\n",
    "test_pred_prob = model.predict_proba(test_X_treated)[:,1]\n",
    "\n",
    "roc_auc_score(test_Y, test_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load submit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_data = pd.read_csv('data/test_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train and submit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32369524, 15), (32369524,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X = train_data.iloc[:,:-1]\n",
    "train_data_Y = train_data.iloc[:,-1]\n",
    "train_data_X.shape, train_data_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X_treated = pipeline_full.fit_transform(train_data_X)\n",
    "train_data_X_treated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_data_treated = pipeline_full.transform(submit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = current_model()\n",
    "model.fit(train_data_X_treated, train_data_Y)\n",
    "pred_prob = model.predict_proba(submit_data_treated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catch_the_fraudster3",
   "language": "python",
   "name": "catch_the_fraudster3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
